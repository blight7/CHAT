import os
import streamlit as st
import time
import logging
import boto3
import torch
import json
from pymilvus import connections, Collection, utility
from transformers import AutoTokenizer, AutoModel
from botocore.config import Config

# ‡∏õ‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏à‡πâ‡∏á‡πÄ‡∏ï‡∏∑‡∏≠‡∏ô‡∏à‡∏≤‡∏Å Torch ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
os.environ["TORCH_CPP_LOG_LEVEL"] = "ERROR"

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Constants
MILVUS_HOST = "milvus-standalone"
MILVUS_PORT = "19530"
MILVUS_COLLECTION_NAME = "law"
BEDROCK_REGION = "us-east-1"
BEDROCK_MODEL_ID = "anthropic.claude-3-5-sonnet-20240620-v1:0"  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Model ID ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
MODEL_PATH = "/app/BGE-M3"  # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ path ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á
DIMENSION = 1024
MAX_RETRIES = 3
RETRY_DELAY = 1.5

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå (GPU ‡∏´‡∏£‡∏∑‡∏≠ CPU)
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
logger.info(f"üöÄ ‡πÉ‡∏ä‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå: {DEVICE}")

#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Embedding (cached)
#######################################
@st.cache_resource(show_spinner=False)
def load_embedding_model():
    try:
        logger.info(f"üöÄ ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Embedding ‡∏ö‡∏ô‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå: {DEVICE}")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            cache_dir="/tmp",
            trust_remote_code=True
        )
        model = AutoModel.from_pretrained(
            MODEL_PATH,
            local_files_only=True,
            cache_dir="/tmp",
            trust_remote_code=True
        ).to(DEVICE)
        logger.info("‚úÖ ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")

        if DEVICE.type == "cuda":
            try:
                # model = torch.compile(model)  # Uncomment ‡∏ñ‡πâ‡∏≤‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏°‡∏µ C compiler
                logger.info("‚úÖ torch.compile ‡∏ú‡πà‡∏≤‡∏ô (‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ)")
            except Exception as compile_error:
                logger.warning(f"torch.compile ‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß: {compile_error}. ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•")
            model = model.half()  # ‡∏•‡∏î precision ‡πÄ‡∏õ‡πá‡∏ô 16-bit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU

            # ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢ dummy input
            dummy_input = tokenizer("‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö", return_tensors="pt", padding=True, truncation=True, max_length=512)
            dummy_input = {k: v.to(DEVICE) for k, v in dummy_input.items()}
            with torch.no_grad():
                _ = model(**dummy_input)

        model.eval()
        logger.info(f"‚úÖ ‡πÇ‡∏´‡∏•‡∏î BGE-M3 ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ö‡∏ô {DEVICE}")
        return tokenizer, model
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        return None, None

#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠ Milvus (retry ‡πÅ‡∏•‡∏∞ cached)
#######################################
def milvus_connection_with_retry():
    for attempt in range(MAX_RETRIES):
        try:
            connections.connect(alias="default", host=MILVUS_HOST, port=MILVUS_PORT)
            if not utility.has_collection(MILVUS_COLLECTION_NAME):
                logger.error(f"Collection not found: {MILVUS_COLLECTION_NAME}")
                return None
            collection = Collection(MILVUS_COLLECTION_NAME)

            # üîπ ‡πÇ‡∏´‡∏•‡∏î Collection ‡πÄ‡∏Ç‡πâ‡∏≤‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
            collection.load()  # <== ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ

            logger.info(f"‚úÖ Connected to Milvus and loaded collection on attempt {attempt + 1}")
            return collection
        except Exception as e:
            logger.warning(f"Connection attempt {attempt + 1} failed: {e}")
            time.sleep(RETRY_DELAY ** (attempt + 1))
    return None


@st.cache_resource
def connect_to_milvus():
    collection = milvus_connection_with_retry()
    if collection is None:
        logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ö Milvus ‡πÑ‡∏î‡πâ")
    return collection

def test_milvus_search(collection, tokenizer, model):
    test_text = "‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö"
    logger.info(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {test_text}")
    embedding = generate_embedding(test_text, tokenizer, model)
    if not embedding:
        logger.error("‚ùå ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à")
        return
    results = search_milvus(collection, embedding, top_k=3)
    if results is None:
        logger.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Milvus ‡πÑ‡∏î‡πâ")
    else:
        processed = process_results(results)
        logger.info(f"‚úÖ ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö: {processed}")

#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î Bedrock LLM Client (cached)
#######################################
@st.cache_resource(show_spinner=False)
def get_llm_client():
    try:
        client = boto3.client(
            service_name="bedrock-runtime",
            region_name=BEDROCK_REGION,
            config=Config(connect_timeout=5, read_timeout=60)
        )
        logger.info("‚úÖ Initialized Bedrock LLM client")
        return client
    except Exception as e:
        logger.error(f"‚ùå Error initializing LLM client: {e}")
        return None


#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á Embedding
#######################################
def generate_embedding(text, tokenizer, model):
    try:
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        logger.info(f"Input tokens: {inputs}")
        inputs = {k: v.to(DEVICE) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
            # ‡πÉ‡∏ä‡πâ mean pooling ‡∏Ç‡∏≠‡∏á hidden states ‡πÄ‡∏õ‡πá‡∏ô embedding
            embedding = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()[0].tolist()
        logger.info(f"Embedding dimension: {len(embedding)} (‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á: {DIMENSION})")
        return embedding
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á embedding: {e}")
        return None


#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Milvus
#######################################
def search_milvus(collection, query_embedding, top_k=5):
    try:
        # logger.info("üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏° flush collection ‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤")
        # collection.flush()  # ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏ñ‡∏π‡∏Å flush
        search_params = {"metric_type": "COSINE", "params": {"nprobe": 16}}
        logger.info(f"üîç ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Milvus ‡∏î‡πâ‡∏ß‡∏¢‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå: {search_params}")
        start_time = time.time()
        results = collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            output_fields=["law_name", "year", "section", "content"],
            consistency_level="Bounded"
        )
        elapsed_time = time.time() - start_time
        logger.info(f"‚úÖ ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Milvus ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡πÉ‡∏ô {elapsed_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ")
        if results:
            total_hits = sum(len(hits) for hits in results)
            logger.info(f"‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {total_hits} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£")
        else:
            logger.info("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Milvus")
        return results
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Milvus: {e}")
        return None

#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
#######################################
def process_results(search_results):
    try:
        return [
            {
                "law_name": getattr(hit.entity, "law_name", "N/A"),
                "year": getattr(hit.entity, "year", "N/A"),
                "section": getattr(hit.entity, "section", "N/A"),
                "content": getattr(hit.entity, "content", "N/A"),
                "score": hit.score
            }
            for hits in search_results for hit in hits
        ] if search_results else []
    except Exception as e:
        logger.error(f"‚ùå ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå: {e}")
        return []

#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM (Bedrock) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
#######################################
def generate_response(llm_client, query, context, category):
    if not context:
        return "‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•"
    prompt_template = f"""
<‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó> ‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÑ‡∏ó‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞ </‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó>
<‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á> {context} </‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á>
<‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°> {query} </‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°>
<‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î>
1. ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
2. ‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡∏∑‡πà‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
3. ‡∏´‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏û‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏à‡πâ‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
4. ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
5. ‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏®‡∏±‡∏û‡∏ó‡πå‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
6. ‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö‡∏û‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Ç‡∏õ
</‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î>
"""
    payload = {
        "messages": [
            {"role": "user", "content": prompt_template}
        ],
        "max_tokens": 500,  # ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô token ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ï‡∏≠‡∏ö
        "anthropic_version": "bedrock-2023-05-31"  # ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏ (‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà Bedrock ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
    }
    try:
        logger.info("üì® ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ‡∏¢‡∏±‡∏á LLM ‡∏ú‡πà‡∏≤‡∏ô Bedrock...")
        response = llm_client.invoke_model(
            modelId=BEDROCK_MODEL_ID,
            contentType="application/json",
            accept="application/json",
            body=json.dumps(payload)
        )
        # ‡∏≠‡πà‡∏≤‡∏ô response ‡πÅ‡∏•‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict
        response_body = response["body"].read().decode("utf-8")
        result = json.loads(response_body)

        # Logging ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á response ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå
        logger.info(f"LLM raw response: {json.dumps(result, ensure_ascii=False, indent=2)}")

        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏û‡∏ö (‡πÉ‡∏ä‡πâ key "content" ‡πÅ‡∏ó‡∏ô "completions")
        content = result.get("content", [])
        if content and isinstance(content, list):
            # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å dictionary ‡πÅ‡∏£‡∏Å‡πÉ‡∏ô list
            answer = content[0].get("text", "")
            if answer:
                return answer
        return "‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å LLM ‡πÑ‡∏î‡πâ"
    except Exception as e:
        logger.error(f"‚ùå Error generating response: {e}")
        return "‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö"


#######################################
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
#######################################
def load_all():
    tokenizer, model = load_embedding_model()
    collection = connect_to_milvus()
    llm_client = get_llm_client()
    return tokenizer, model, collection, llm_client

# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ S3
def setup_s3_client():
    if "s3_client" not in st.session_state:
        st.session_state.s3_client = boto3.client('s3')

def list_files_in_s3(bucket_name):
    try:
        objects = st.session_state.s3_client.list_objects_v2(Bucket=bucket_name)
        return [obj['Key'] for obj in objects.get('Contents', [])] if 'Contents' in objects else []
    except Exception as e:
        st.error(f"Error listing files: {e}")
        return []

def upload_to_s3(file, bucket_name, file_name):
    try:
        sanitized_file_name = file_name.replace(" ", "_")  # ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå
        st.session_state.s3_client.upload_fileobj(file, bucket_name, sanitized_file_name)
        return True
    except Exception as e:
        st.error(f"Error uploading file: {e}")
        return False

#######################################
# ‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ (Streamlit UI)
#######################################
def main():
    st.set_page_config(page_title="‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏ó‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£", page_icon="‚öñÔ∏è", layout="wide")
    st.title("‡πÅ‡∏ä‡∏ó‡∏ö‡∏≠‡∏ó‡∏ó‡∏ô‡∏≤‡∏¢ üìú")

    st.title("Main Page")

########################################
#‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÑ‡∏ã‡∏ï‡πå‡∏ö‡∏≤‡∏£‡πå (Sidebar)
########################################
    with st.sidebar:
        st.header("üóÇÔ∏è ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó")

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö state ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = {}
        if "current_chat" not in st.session_state:
            st.session_state.current_chat = None
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ‡∏õ‡∏∏‡πà‡∏°‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡∏°‡πà
        if st.button("‚ûï ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡∏°‡πà"):
            new_chat_id = f"‡πÅ‡∏ä‡∏ó {len(st.session_state.chat_history) + 1}"
            st.session_state.chat_history[new_chat_id] = []
            st.session_state.current_chat = new_chat_id
            st.session_state.messages = []
            st.success("‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó‡∏ó‡∏µ‡πà‡∏°‡∏µ ‡πÅ‡∏•‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ä‡∏ó
        if st.session_state.chat_history:
            chat_names = list(st.session_state.chat_history.keys())
            selected_chat = st.selectbox("üîç ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ä‡∏ó", chat_names,
                                         index=chat_names.index(st.session_state.current_chat)
                                         if st.session_state.current_chat in chat_names else 0)

            st.session_state.current_chat = selected_chat
            st.session_state.messages = st.session_state.chat_history[selected_chat]

            # ‡∏õ‡∏∏‡πà‡∏°‡∏•‡∏ö‡πÅ‡∏ä‡∏ó (‡∏°‡∏µ‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡∏Å‡∏±‡∏ô Error)
            if st.button("üóëÔ∏è ‡∏•‡∏ö‡πÅ‡∏ä‡∏ó‡∏ô‡∏µ‡πâ"):
                if selected_chat in st.session_state.chat_history:
                    del st.session_state.chat_history[selected_chat]

                    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÅ‡∏ä‡∏ó‡πÉ‡∏´‡∏°‡πà‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å‡∏•‡∏ö
                    if st.session_state.chat_history:
                        st.session_state.current_chat = list(st.session_state.chat_history.keys())[0]
                    else:
                        st.session_state.current_chat = None
                        st.session_state.messages = []

                    st.success(f"‡∏•‡∏ö‡πÅ‡∏ä‡∏ó‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
                else:
                    st.warning("‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏•‡∏ö‡πÅ‡∏ä‡∏ó‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ!")

        # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó‡∏Ç‡∏≠‡∏á‡πÅ‡∏ä‡∏ó‡∏ó‡∏µ‡πà‡πÄ‡∏•‡∏∑‡∏≠‡∏Å
        if st.session_state.chat_history and st.session_state.current_chat:
            st.subheader("üí¨ ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó")
            current_chat = st.session_state.current_chat
            if current_chat in st.session_state.chat_history:
                for role, content in st.session_state.chat_history[current_chat]:  # ‚úÖ ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ tuple unpacking
                    st.write(f"{role}: {content}")
            else:
                st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó")
        else:
            st.write("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó")

        # ‡∏õ‡∏∏‡πà‡∏°‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó
        if st.button("üîÑ ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó"):
            st.session_state.chat_history = {}
            st.session_state.messages = []
            st.session_state.current_chat = None
            st.success("‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß")

    st.subheader("üìÇ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£")
    uploaded_files = st.file_uploader("‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå PDF", type=["pdf"], accept_multiple_files=True)
    if uploaded_files:
        for uploaded_file in uploaded_files:
            file_name = uploaded_file.name
            st.write(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î {file_name}...")
            if upload_to_s3(uploaded_file, "lmskm", file_name):
                st.success(f"‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î {file_name} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

    st.subheader("‡∏ñ‡∏≤‡∏°‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢")
    category = st.selectbox("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà",
                            ["‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏≤‡∏°‡∏ô‡∏∏‡∏©‡∏¢‡πå", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ ‡∏û.‡∏£.‡∏ö ‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå", "‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏£‡∏±‡∏ê‡∏ò‡∏£‡∏£‡∏°‡∏ô‡∏π‡∏ç"])

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° session state
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = {}
    if "current_chat" not in st.session_state:
        st.session_state.current_chat = "default_chat"
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for role, content in st.session_state.messages:
        with st.chat_message(role):
            st.markdown(content)

    # ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏û‡∏£‡πâ‡∏≠‡∏° spinner
    with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£..."):
        tokenizer, model, collection, llm_client = load_all()

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö Milvus (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏õ‡∏±‡∏ç‡∏´‡∏≤)
    if collection and tokenizer and model:
        test_milvus_search(collection, tokenizer, model)

    if not (tokenizer and model and collection and llm_client):
        st.error("‡∏£‡∏∞‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏ó‡∏£‡∏±‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î")
        return

    # ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    query = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:")
    if query:
        st.session_state.messages.append(("user", query))
        with st.chat_message("user"):
            st.markdown(query)
            st.session_state['start_time'] = time.time()

        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏à‡∏≤‡∏Å query (‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç: ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á category ‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô generate_embedding)
        with st.spinner("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
            embedding = generate_embedding(query, tokenizer, model)
        if not embedding:
            st.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏õ‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ")
            return

        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô Milvus ‡πÇ‡∏î‡∏¢‡∏Å‡∏≥‡∏´‡∏ô‡∏î top_k=5 (‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
        search_results = search_milvus(collection, embedding, top_k=5)
        processed_results = process_results(search_results)
        if not processed_results:
            st.warning("‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á")
            return

        # ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏õ‡πá‡∏ô context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        context = "\n\n".join([
            f"""üìú **‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢:** {res['law_name']} (üìÖ ‡∏õ‡∏µ: {res['year']})  
            üîñ **‡∏°‡∏≤‡∏ï‡∏£‡∏≤:** {res['section']}  
            üìù **‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:** {res['content']}  
            üéØ **‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô:** {res['score']:.4f}  
            ---"""
                for res in processed_results
        ])

        with st.spinner("üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•..."):
            # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ LLM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ context ‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å Milvus
            response = generate_response(llm_client, query, context, category)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏≤‡∏Å‡∏£‡∏∞‡∏ö‡∏ö (LLM)
            with st.chat_message("assistant"):
                st.markdown(response)
            st.session_state.messages.append(("assistant", response))
            st.session_state.chat_history[st.session_state.current_chat] = list(st.session_state.messages)

        # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Top 5 ‡∏à‡∏≤‡∏Å Milvus
        st.subheader("üîç ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ (Top 5)")
        for res in processed_results:
            st.markdown(
                f"""üìú **‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢:** {res['law_name']} (üìÖ ‡∏õ‡∏µ: {res['year']})  
        üîñ **‡∏°‡∏≤‡∏ï‡∏£‡∏≤:** {res['section']}  
        üìù **‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤:** {res['content']}  
        üéØ **‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô:** {res['score']:.4f}  
        ---
        """
            )

# ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    main()

# Logging ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á GPU
logger.info(f"CUDA Available in code: {torch.cuda.is_available()}")
logger.info(f"Using device: {DEVICE}")
